# questionCaptionMatchModels

Code for identifying True vs False Premise questions for a given image as described in the paper, Question Relevance in VQA:
Identifying Non-Visual And False-Premise Questions,  https://arxiv.org/pdf/1606.06622v2.pdf

### Pre-requisites:

- Spacy: 
  - ```pip install spacy```
  - ```sputnik --name spacy --repository-url http://index.spacy.io install en==1.1.0```

- Keras Deep Learning Library:
  - ```pip install keras==0.3.2``` (make sure to include the ==0.3.2 part since that installs version 0.3.2. The code might crash with other versions since some layers used in the code might be deprecated/ implemented differently in other versions.)

- Scikit Learn Package
  - ```pip install -U scikit-learn```

- h5py Package
  - ```pip install h5py```

- Gensim Package
  - ```pip install -U gensim```

(NOTE: You might need sudo access for any/all of the above. So, to install with sudo, just put ```sudo``` behind every command. If you do not have sudo, you can install all of the above in a virtual environment (see http://docs.python-guide.org/en/latest/dev/virtualenvs/).) 

Phew, a lot of requirements! Sorry, no way around it!

### How to run the code:

```python questionCaptionModel.py --captype [option1] --model [option2] --loadweights [option3] --saveModel [option4]```

Options are:

- [option1] :
  - ```qc``` for question-caption similarity.
  - ```qq``` for question-question similarity.
  - ```qdq``` This method is not described in the paper and is an experimental method where the similarity is computed between the test question and a set of diverse questions generated by a captioning model. 
  
- [option2] :
  - ```bow``` : concatenates test question and generated question/caption features by Bag-of-Words technique 
  - ```avgw2v``` : concatenates test question and generated question/caption features by averaging word2vec.
  - ```lstm``` : concatenates test question and generated question/caption features by feeding in the word2vec vectors into an lstm
  
- [option3] : (optional) Path of pretrained weights. You will find some pretrained weights in the ```outputmodels/``` folder. The final paper reports the average performance of multiple runs to alleviate any accuracy spikes due to training noise. Following are the one of the weights used in computing the average in the final paper:
  - Question-Caption Match:
    - BoW:  ```outputmodels/2016-08-24\ 02\:51\:48.482004_qc_bow.h5```
    - Avg Word2Vec:  ```outputmodels/2016-08-24\ 02\:47\:17.626481_qc_avgw2v.h5```
    - LSTM:  ```outputmodels/2016-08-24\ 03\:32\:01.409005_qc_lstm.h5 ```
  - Question-Question Match:
    - BoW:   ```outputmodels/2016-08-24\ 02\:29\:46.027887_qq_bow.h5```
    - Avg Word2Vec: ```outputmodels/2016-08-24\ 02\:19\:07.961071_qq_avgw2v.h5```
    - LSTM:  ```outputmodels/2016-08-24\ 11\:54\:29.343364_qq_lstm.h5```

- [option4] : If [option3] is not specified, the training is executed. Set ```FALSE``` to NOT save your trained model at the end of execution, default value is ```TRUE``` (i.e trained model will be saved)

NOTE: The Questions/Captions for the images in the Visual True vs False Premise data for computing similarity with the test/train questions have already been pre-generated here. If you want to run it on your own set of images and test/train questions, you have to generate the captions/questions for the images using a caption generator. I used Andrej Karpathy's NeuralTalk2. Code and instructions for generating captions in the format our models need will be uploaded soon. Please email me if you need them immediately.  

Please contact ray93@vt.edu if you have any questions.
