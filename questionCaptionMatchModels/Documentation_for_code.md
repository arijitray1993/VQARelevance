# Documentation for questionCaptionModel.py file

let's start with avg_w2v features type:

In main() method, "run_avg_w2v_model" method is called

load_data is called

  1. Generated Captions

    For qc / qq , imagecaptions.json/imagecaptions_as_ques.json file is accessed which contains data as follows:

    {"imgid":"caption generated by pre-trained NeuralTalk2","img2id":"caption generated by pre-trained NeuralTalk2"}

    {"imgid":"question generated by pre-trained NeuralTalk2","img2id":"question generated by pre-trained NeuralTalk2"}

    We keep this in "image_captions"

  2. QI compatibility = Annotated by AMT workers

    "RamScoresNew.json" stores the information as follows:

    {"15152":{"image":"/home/deep/Projects/Datasets/mscoco/mscoco/images/val2014/COCO_val2014_000000355817.jpg","processed":"true","label":"2","question":"is he wearing pants","num_words":4,"score":-2.7634997367859},
    "3442":{"image":"/home/deep/Projects/Datasets/mscoco/mscoco/images/val2014/COCO_val2014_000000518908.jpg","processed":"true","label":"2","question":"what is covering the windows in the background","num_words":8,"score":-6.8578248023987}}

    Label : "1" means Makes Sense
    Label : "2" means Doesn't Make Sense
    Every image is marked by 3 people .

    qi_applicable consists of all the data_dim i.e "question+image" has more than 1 instances.

    A total of 32139 QI pairs


    unique_qi_applicable filters out the votes i.e. It averages the votes by people and

    qi_applicable has 32139 data, but there should be 10793 unique data as each Q,I pair was answered by 3 workers.

    So , unique_qi_applicable has 10793 instances

    "qi_applicable" & "unique_qi_applicable" are variables to store the above data

  3. Vocab - Nothing to explain
  4. invertvocab - word to index Dictionary(Mapping)

  return 1,2,3,4

##
Now "feature_extract_word2vec" is called

  unique_qi_applicable (Questions with relevance w.r.t images ) & image_captions (Captions generated w.r.t images) are passed
  as arguments

  In "feature_extract_word2vec" function ,

  for every question/caption,
    Tokenize the sentence and create a matrix of word2vec word vectors
    Take average of word vectors in a sentence to get a single vector for sentence

  Map question with caption based on the ImageID common to both.

  concatenate question with caption for every image.

  Store these pairs in "ques_cap_features" variable

  Convert the relevance (1,2) to (1,0)
  0 == Not Relevant
  1 == Relevance

  Store this data in "applicable_labels"

  return "ques_cap_features","applicable_labels"
